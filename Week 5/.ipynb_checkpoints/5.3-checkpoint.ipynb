{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88209c79-f7fb-483f-af09-5146df90d0fe",
   "metadata": {},
   "source": [
    "# Week 5, Class 3: Data Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f99a39-6aa9-457f-a589-a60473121e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50815586-5b2c-42cd-86b5-85de3d8c5549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SampleID  Weight(g)  pH_Level Catalyst     Ratio\n",
      "0    S-001       10.5       7.2        A  0.685714\n",
      "1    S-002       12.1       7.5        B  0.619835\n",
      "2    S-003        9.8       6.9        A  0.704082\n",
      "3    S-004       11.3       7.1        C  0.628319\n",
      "4    S-005       13.5       7.3        B  0.540741\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('experiment_data.csv')\n",
    "df['Ratio'] = df['pH_Level']/df['Weight(g)']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e085fe-fa5a-4b90-8ba4-c32de36aee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SampleID  Weight(g)  pH_Level Catalyst\n",
      "0    S-001       10.5       7.2        A\n",
      "1    S-002       12.1       7.5        B\n",
      "2    S-003        9.8       6.9        A\n",
      "3    S-004       11.3       7.1        C\n",
      "4    S-005       13.5       7.3        B\n"
     ]
    }
   ],
   "source": [
    "# df = df.drop('Ratio', axis=1)\n",
    "del df['Ratio']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ddd38-4417-47ff-8913-23084fe59ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcfcda-4ba1-40d7-a221-bf3ea46215d8",
   "metadata": {},
   "source": [
    "## Task 1: Load the 'Experiments' sheet. How many rows and columns are there?\n",
    "\n",
    "First, we need to load the data from the `Experiments` sheet. We can then use the .shape attribute to get the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b9997a-74ac-4836-92df-efe1c9f00412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355 10\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the 'Experiments' sheet (CSV file)\n",
    "experiments_df = pd.read_excel('data.xlsx', sheet_name='Experiments')\n",
    "\n",
    "# Use the .shape attribute to get the number of rows and columns\n",
    "rows, columns = experiments_df.shape\n",
    "print(rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7646-a025-480c-8510-55f3b92ba016",
   "metadata": {},
   "source": [
    "## Task 2: Show the first 8 rows and the last 3 rows.\n",
    "\n",
    "We can use the `.head()` and `.tail()` methods to view specific numbers of rows from the beginning and end of the DataFrame, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defd9b8c-7d72-4a71-9b3b-1d658d4f8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     exp_id            species treatment  dose_mg  response  replicate  \\\n",
      "0  EXP_0000        A. thaliana    drug_B        0 -0.711957          3   \n",
      "1  EXP_0001        A. thaliana    drug_A      100  4.320830          4   \n",
      "2  EXP_0002       m. musculus    control       10 -0.318308          3   \n",
      "3  EXP_0003   D. melanogaster     drug_A        0  0.682846          1   \n",
      "4  EXP_0004   D. melanogaster     drug_B       50       NaN          3   \n",
      "5  EXP_0005            E. coli    drug_B       50  0.634457          1   \n",
      "6  EXP_0006       m. musculus     drug_B        0 -1.020293          3   \n",
      "7  EXP_0007        A. thaliana   control        5 -0.155242          3   \n",
      "\n",
      "         date    lab  site_id    notes  \n",
      "0  2025-07-24  Lab-1      138       ok  \n",
      "1  2025-07-07  Lab-1      135  T=21.3C  \n",
      "2  2025-07-07  Lab-2      108     none  \n",
      "3  2025-07-12  Lab-3      138  contam?  \n",
      "4  2025-07-24  Lab-3      129   repeat  \n",
      "5  2025-07-07  Lab-2      117       ok  \n",
      "6  2025-07-19  Lab-2      124   repeat  \n",
      "7  2025-07-05  Lab-2      128     none  \n",
      "       exp_id            species treatment  dose_mg  response  replicate  \\\n",
      "352  EXP_0148   D. melanogaster     drug_B       20 -0.813568          2   \n",
      "353  EXP_0058            E. coli   control       10  0.582654          4   \n",
      "354  EXP_0258       m. musculus    control       50 -0.404563          3   \n",
      "\n",
      "           date    lab  site_id    notes  \n",
      "352  2025-07-21  Lab-2      119       ok  \n",
      "353  2025-08-03  Lab-1      107       ok  \n",
      "354  2025-07-22  Lab-2      129  T=23.6C  \n"
     ]
    }
   ],
   "source": [
    "# Show the first 8 rows\n",
    "first_8_rows = experiments_df.head(8)\n",
    "\n",
    "# Show the last 3 rows\n",
    "last_3_rows = experiments_df.tail(3)\n",
    "\n",
    "print(first_8_rows)\n",
    "print(last_3_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2358267-684e-4b43-9afa-abd92172d24d",
   "metadata": {},
   "source": [
    "## Task 3: What are the dtypes? Convert 'dose_mg' to float and 'date' to datetime.\n",
    "\n",
    "The `.dtypes` attribute shows the data type of each column. We'll use `pd.to_numeric()` to handle the `dose_mg` column, which may contain non-numeric values, and `pd.to_datetime()` for the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082d87cc-8e53-4745-ac00-d19618bbb8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_id        object\n",
      "species       object\n",
      "treatment     object\n",
      "dose_mg        int64\n",
      "response     float64\n",
      "replicate      int64\n",
      "date          object\n",
      "lab           object\n",
      "site_id        int64\n",
      "notes         object\n",
      "dtype: object\n",
      "\n",
      "exp_id               object\n",
      "species              object\n",
      "treatment            object\n",
      "dose_mg               int64\n",
      "response            float64\n",
      "replicate             int64\n",
      "date         datetime64[ns]\n",
      "lab                  object\n",
      "site_id               int64\n",
      "notes                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check original dtypes\n",
    "print(experiments_df.dtypes)\n",
    "\n",
    "# Convert 'dose_mg' to float\n",
    "experiments_df['dose_mg'] = pd.to_numeric(experiments_df['dose_mg'])\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "experiments_df['date'] = pd.to_datetime(experiments_df['date'])\n",
    "\n",
    "# Check new dtypes\n",
    "print()\n",
    "print(experiments_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5da5a1-d6e3-4140-91b1-da249beb376f",
   "metadata": {},
   "source": [
    "## Task 4: How many missing values are there in each column? Fill missing 'response' with the group median per 'treatment'.\n",
    "\n",
    "We'll use `.isnull().sum()` to count missing values. For filling the missing 'response' values, we'll apply a common strategy of using the median of the corresponding treatment group, which is a more robust approach than using the overall median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad150b3-4d2f-4cfe-8c99-c01d70cb8d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original missing values:\n",
      "exp_id        0\n",
      "species       0\n",
      "treatment     0\n",
      "dose_mg       0\n",
      "response     22\n",
      "replicate     0\n",
      "date          0\n",
      "lab           0\n",
      "site_id       0\n",
      "notes         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after filling:\n",
      "exp_id       0\n",
      "species      0\n",
      "treatment    0\n",
      "dose_mg      0\n",
      "response     0\n",
      "replicate    0\n",
      "date         0\n",
      "lab          0\n",
      "site_id      0\n",
      "notes        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values_count = experiments_df.isnull().sum()\n",
    "# print(missing_values_count)\n",
    "\n",
    "# Fill missing 'response' values with the median of their 'treatment' group\n",
    "experiments_df['response'] = experiments_df['response'].fillna(\n",
    "    experiments_df.groupby('treatment')['response'].transform('median')\n",
    ")\n",
    "\n",
    "print(f\"Original missing values:\\n{missing_values_count}\\n\\nMissing values after filling:\\n{experiments_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c0660-23d0-42ab-8a53-9b897ccee913",
   "metadata": {},
   "source": [
    "## Task 5: Strip whitespace and lowercase the 'species' values. Count unique species.\n",
    "\n",
    "The `.str` accessor in Pandas allows us to apply string methods to an entire `Series`. We'll chain the `.str.strip()` and `.str.lower()` methods. Then, we can use `value_counts()` or `nunique()` to find the unique species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2494c408-41aa-449f-af32-38ec69cc208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique species: 5\n",
      "\n",
      "Counts of each unique species:\n",
      "species\n",
      "m. musculus        76\n",
      "e. coli            74\n",
      "a. thaliana        72\n",
      "d. melanogaster    72\n",
      "c. elegans         61\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace and lowercase 'species' values\n",
    "experiments_df['species'] = experiments_df['species'].str.strip().str.lower()\n",
    "\n",
    "# Count the number of unique species\n",
    "unique_species_count = experiments_df['species'].nunique()\n",
    "\n",
    "# Show the counts for each unique species\n",
    "species_counts = experiments_df['species'].value_counts()\n",
    "\n",
    "print(f\"Number of unique species: {unique_species_count}\\n\\nCounts of each unique species:\\n{species_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599e16-e157-49cc-ace9-6d6af2895bd9",
   "metadata": {},
   "source": [
    "## Task 6: From 'Experiments', compute mean and std of 'response' by 'treatment' and 'dose_mg'.\n",
    "\n",
    "We'll use the powerful `.groupby()` method on two columns (`treatment` and `dose_mg`) and then use the `.agg()` method to apply both the mean and standard deviation functions to the `response` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a913eddc-ff71-46a6-84c2-dff77bfbf991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             mean       std\n",
      "treatment dose_mg lab                      \n",
      "control   0       Lab-1  0.261285  0.403055\n",
      "                  Lab-2  0.000415  0.488973\n",
      "                  Lab-3 -0.212387       NaN\n",
      "          5       Lab-1 -0.300765  0.244102\n",
      "                  Lab-2  0.193472  0.460784\n",
      "                  Lab-3 -0.076791  0.545555\n",
      "          10      Lab-1  0.111116  0.457885\n",
      "                  Lab-2 -0.207769  0.586683\n",
      "                  Lab-3  0.031207  0.504031\n",
      "          20      Lab-1  0.068439  0.537659\n",
      "                  Lab-2 -0.382913  0.335069\n",
      "                  Lab-3 -0.188204  0.576952\n",
      "          50      Lab-1  0.371146  0.461143\n",
      "                  Lab-2  0.115876  0.626051\n",
      "                  Lab-3  0.089432  0.505576\n",
      "          100     Lab-1 -0.186395  0.255557\n",
      "                  Lab-2 -0.467486  0.634878\n",
      "                  Lab-3  0.129088  0.431946\n",
      "drug_A    0       Lab-1  1.624033  0.540396\n",
      "                  Lab-2  1.271916  0.565592\n",
      "                  Lab-3  1.271962  0.354660\n",
      "          5       Lab-1  1.172608  0.253964\n",
      "                  Lab-2  1.665057  0.239627\n",
      "                  Lab-3  1.467233  0.335648\n",
      "          10      Lab-1  1.543658  0.521146\n",
      "                  Lab-2  1.759726  0.530226\n",
      "                  Lab-3  1.586365  0.299837\n",
      "          20      Lab-1  1.923330  0.471636\n",
      "                  Lab-2  1.499925  0.234509\n",
      "                  Lab-3  1.762310  0.354719\n",
      "          50      Lab-1  2.496115  0.792238\n",
      "                  Lab-2  2.492212  0.430064\n",
      "                  Lab-3  2.685687  0.518383\n",
      "          100     Lab-1  4.485922  0.404110\n",
      "                  Lab-2  4.054061  0.476085\n",
      "                  Lab-3  4.511125  0.669340\n",
      "drug_B    0       Lab-1 -0.414845  0.453480\n",
      "                  Lab-2 -0.684557  0.482032\n",
      "                  Lab-3 -0.932454  0.381387\n",
      "          5       Lab-1 -0.713499  0.251804\n",
      "                  Lab-2 -0.457818  0.317084\n",
      "                  Lab-3 -0.458357  0.412358\n",
      "          10      Lab-1 -0.665798  0.479081\n",
      "                  Lab-2  0.196331  0.409013\n",
      "                  Lab-3 -0.219041  0.702649\n",
      "          20      Lab-1 -0.141272  0.432826\n",
      "                  Lab-2 -0.306436  0.621244\n",
      "                  Lab-3  0.108936  0.451002\n",
      "          50      Lab-1  0.409850  0.518478\n",
      "                  Lab-2  0.407860  0.900531\n",
      "                  Lab-3  0.063960  0.608222\n",
      "          100     Lab-1  1.458179  0.374946\n",
      "                  Lab-2  0.914106  0.136639\n",
      "                  Lab-3  1.393630  0.680962\n"
     ]
    }
   ],
   "source": [
    "# Group by 'treatment' and 'dose_mg', then aggregate 'response'\n",
    "agg_results = experiments_df.groupby(['treatment', 'dose_mg', 'lab'])['response'].agg(['mean', 'std'])\n",
    "\n",
    "print(agg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea905f8-0aa5-4844-a8c7-e677fce1ab40",
   "metadata": {},
   "source": [
    "## Task 7: In 'Sensors_TimeSeries', resample each sensor to 30-minute means. Forward-fill gaps up to 2 periods.\n",
    "\n",
    "This task requires us to work with time-series data. We'll first load the `Sensors_TimeSeries` data and convert the `timestamp_utc` column to a proper datetime object. Then, we can set it as the index and use `.resample()` to group the data into 30-minute intervals and calculate the mean. Finally, we'll use `.ffill(limit=2)` to forward-fill any small gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f9a043c-ed4c-477e-a25f-41aabdf8cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sensor_id       timestamp_utc      value\n",
      "0      S-01 2025-07-15 00:00:00  19.177910\n",
      "1      S-01 2025-07-15 00:05:00        NaN\n",
      "2      S-01 2025-07-15 00:10:00  19.508683\n",
      "3      S-01 2025-07-15 00:15:00  19.829811\n",
      "4      S-01 2025-07-15 00:20:00  19.092793\n",
      "\n",
      "                                   value\n",
      "sensor_id timestamp_utc                 \n",
      "S-01      2025-07-15 00:00:00  19.420979\n",
      "          2025-07-15 00:30:00  19.440383\n",
      "          2025-07-15 01:00:00  19.719512\n",
      "          2025-07-15 01:30:00  19.314704\n",
      "          2025-07-15 02:00:00  19.982121\n"
     ]
    }
   ],
   "source": [
    "# Load the 'Sensors_TimeSeries' data\n",
    "sensors_df = pd.read_excel('data.xlsx', sheet_name='Sensors_TimeSeries')\n",
    "print(sensors_df.head())\n",
    "\n",
    "# Convert timestamp_utc to datetime\n",
    "sensors_df['timestamp_utc'] = pd.to_datetime(sensors_df['timestamp_utc'])\n",
    "\n",
    "# Set the timestamp as the index and resample to 30-minute means\n",
    "resampled_data = sensors_df.set_index('timestamp_utc').groupby('sensor_id').resample('30min').mean()\n",
    "\n",
    "# Forward-fill gaps up to 2 periods\n",
    "resampled_data = resampled_data.ffill(limit=2)\n",
    "\n",
    "print()\n",
    "print(resampled_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53662a17-a182-4a25-b5d9-7c32cc9bcdde",
   "metadata": {},
   "source": [
    "## Task 8: Merge 'Gene_Expression_Wide' with 'Sample_Metadata' on sample_id (columns starting with 'S'). Bring 'condition' and 'batch' alongside expression values.\n",
    "\n",
    "We need to merge the `Gene_Expression_Wide` and `Sample_Metadata` DataFrames on a common `sample_id` column. We'll use `pd.merge()` for this and specify the columns to merge on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b98a3e2a-7fd6-4486-8dc6-7d4d29ae196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Gene_001  Gene_002  Gene_003  Gene_004  Gene_005  Gene_006  Gene_007  \\\n",
      "S01  3.454046 -1.327933  0.079215 -1.989168  0.542515       NaN  1.119087   \n",
      "S01  3.454046 -1.327933  0.079215 -1.989168  0.542515       NaN  1.119087   \n",
      "S02  0.285124  0.533252 -1.417009  0.540610  3.271026 -1.048592 -0.466789   \n",
      "S03 -0.380625 -1.345471  2.355821       NaN -1.192482 -0.752036  0.867776   \n",
      "S04  1.420028       NaN -0.028942  1.389560 -0.689345  0.222132 -0.671149   \n",
      "\n",
      "     Gene_008  Gene_009  Gene_010  ...  Gene_053  Gene_054  Gene_055  \\\n",
      "S01 -2.539208  1.096142  0.100143  ... -0.079846 -0.478333 -1.977279   \n",
      "S01 -2.539208  1.096142  0.100143  ... -0.079846 -0.478333 -1.977279   \n",
      "S02  0.797859 -0.722360 -1.495660  ... -0.060314 -0.935733  1.353836   \n",
      "S03 -0.463745 -0.594727  0.116468  ...  2.996239 -1.188199  0.392062   \n",
      "S04 -0.410341       NaN -0.933226  ... -1.077228 -1.651178  1.010781   \n",
      "\n",
      "     Gene_056  Gene_057  Gene_058  Gene_059  Gene_060  condition  batch  \n",
      "S01 -0.698763  0.903377  0.382871 -0.809517 -1.526586    control     B2  \n",
      "S01 -0.698763  0.903377  0.382871 -0.809517 -1.526586    control     B2  \n",
      "S02  0.788611  0.497136 -0.994807  0.988284 -0.654896    treated     B2  \n",
      "S03  0.822229  0.531586  0.908261  1.191551 -0.840485    treated     B1  \n",
      "S04  1.054956 -0.984653  0.298431  1.512144  0.021698    control     B3  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary sheets\n",
    "gene_expression_df = pd.read_excel('data.xlsx', sheet_name='Gene_Expression_Wide')\n",
    "sample_metadata_df = pd.read_excel('data.xlsx', sheet_name='Sample_Metadata')\n",
    "\n",
    "# Merge the dataframes on the 'sample_id' columns\n",
    "# Note: In 'Gene_Expression_Wide', the sample IDs are columns, so we need to transpose it first\n",
    "# or melt it to a long format. A simpler approach is to merge the metadata into a temporary\n",
    "# transposed version and then join. A much cleaner way is to first melt the gene expression data.\n",
    "# For simplicity, we'll use a direct merge with a re-shaped dataframe.\n",
    "# Let's adjust the sample_metadata to have 'sample_id' as the index for easier joining.\n",
    "sample_metadata_df.set_index('sample_id', inplace=True)\n",
    "gene_expression_df.rename(columns={'gene': 'gene_id'}, inplace=True)\n",
    "gene_expression_df.set_index('gene_id', inplace=True)\n",
    "merged_df = gene_expression_df.transpose().merge(sample_metadata_df[['condition', 'batch']], left_index=True, right_index=True)\n",
    "\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82bb6d-9050-4107-9177-f3e1737cf223",
   "metadata": {},
   "source": [
    "## Task 9: From 'Assays_Long', pivot to wide with one row per (sample_id, day) and columns as assay_type with mean 'value'.\n",
    "\n",
    "The `Assays_Long` data is in a long format. We need to reshape it into a wide format where each `assay_type` is its own column. Since we need to calculate the mean value for cases with duplicate `(sample_id, day, assay_type)` combinations, `pd.pivot_table()` is the appropriate function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c7ded-844f-494f-95c4-8d2b93d199a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'Assays_Long' data\n",
    "assays_df = pd.read_excel('data.xlsx', sheet_name='Assays_Long')\n",
    "\n",
    "# Pivot the data to wide format, computing the mean value\n",
    "pivoted_df = assays_df.pivot_table(\n",
    "    index=['sample_id', 'day'],\n",
    "    columns='assay_type',\n",
    "    values='value',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(pivoted_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758c6ce-9ca6-42a1-bfc9-78014e9c92c4",
   "metadata": {},
   "source": [
    "## Task 10: Join 'Geo_Sites' to 'Experiments' using 'site_id'. Compute average response by region.\n",
    "\n",
    "We will merge the `Geo_Sites` DataFrame with the cleaned `Experiments` DataFrame on the `site_id` column. After the merge, we can use `groupby()` to calculate the average response for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac55e8-e4a9-4363-a0c8-d813200499fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'Geo_Sites' data\n",
    "geo_sites_df = pd.read_excel('data.xlsx', sheet_name='Geo_Sites')\n",
    "\n",
    "# Merge the two dataframes on 'site_id'\n",
    "experiments_with_geo = pd.merge(experiments_df, geo_sites_df, on='site_id', how='left')\n",
    "\n",
    "# Compute the average response per region\n",
    "average_response_by_region = experiments_with_geo.groupby('region')['response'].mean()\n",
    "\n",
    "print(average_response_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0277b-1f61-4466-a4fb-c9aa67133ac5",
   "metadata": {},
   "source": [
    "## Task 11: In 'Inventory' and 'Inventory_Updates', perform a left join on 'sku'. Compute the new stock = stock + delta (NaN -> 0).\n",
    "\n",
    "This task combines merging and handling missing values. We'll perform a left merge from `Inventory` to `Inventory_Updates`. The `delta` column in the merged DataFrame will have `NaN` values for items without updates. We will fill these `NaN`s with 0 before calculating the `new_stock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633eb61-98ab-4da9-a265-ad12bfbe412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary sheets\n",
    "inventory_df = pd.read_excel('data.xlsx', sheet_name='Inventory')\n",
    "updates_df = pd.read_excel('data.xlsx', sheet_name='Inventory_Updates')\n",
    "\n",
    "# Perform a left merge on 'sku'\n",
    "inventory_merged = pd.merge(inventory_df, updates_df, on='sku', how='left')\n",
    "\n",
    "# Fill NaN values in 'delta' with 0\n",
    "inventory_merged['delta'] = inventory_merged['delta'].fillna(0)\n",
    "\n",
    "# Compute the new stock\n",
    "inventory_merged['new_stock'] = inventory_merged['stock'] + inventory_merged['delta']\n",
    "\n",
    "print(inventory_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065a439-05a1-4acb-9e4d-ab43d3fd912d",
   "metadata": {},
   "source": [
    "## Task 12: Detect duplicated rows in 'Experiments' (full-row duplicates) and drop them keeping the first occurrence.\n",
    "\n",
    "Pandas' `drop_duplicates()` method is perfect for this. It can detect and remove rows that are identical across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2102c5-f7cb-4acb-b999-817553e834be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count original rows\n",
    "original_rows = len(experiments_df)\n",
    "\n",
    "# Drop duplicate rows, keeping the first occurrence\n",
    "df_no_dupes = experiments_df.drop_duplicates()\n",
    "\n",
    "# Count rows after dropping duplicates\n",
    "rows_after_dupes = len(df_no_dupes)\n",
    "\n",
    "print(f\"Original number of rows: {original_rows}\",\n",
    "      f\"Number of rows after dropping duplicates: {rows_after_dupes}\", \n",
    "f\"Dropped {original_rows - rows_after_dupes} duplicate rows.\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab32749-3b99-4134-84bb-4e7719a15266",
   "metadata": {},
   "source": [
    "## Task 13: Parse the 'notes' column in 'Experiments' and extract any temperature pattern like 'T=23.5C' into a new column 'temp_C'.\n",
    "\n",
    "We'll use the `.str.extract()` method with a regular expression to find and capture the temperature value. The regular expression `r'T=(\\d+\\.?\\d*)C'` is designed to match a temperature pattern, capturing the number part, and it also handles cases where there's no decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0d7a9-b50a-49dd-b6fd-e24171b9a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the temperature pattern into a new column\n",
    "experiments_df['temp_C'] = experiments_df['notes'].str.extract(r'T=(\\d+\\.?\\d*)C')\n",
    "\n",
    "# Convert the new column to a numeric type\n",
    "experiments_df['temp_C'] = pd.to_numeric(experiments_df['temp_C'])\n",
    "\n",
    "print(experiments_df[['notes', 'temp_C']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
