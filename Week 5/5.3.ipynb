{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88209c79-f7fb-483f-af09-5146df90d0fe",
   "metadata": {},
   "source": [
    "# Week 5, Class 3: Data Cleaning, and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f99a39-6aa9-457f-a589-a60473121e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50815586-5b2c-42cd-86b5-85de3d8c5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('experiment_data.csv')\n",
    "df['Ratio'] = df['pH_Level']/df['Weight(g)']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e085fe-fa5a-4b90-8ba4-c32de36aee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Ratio', axis=1)\n",
    "# del df['Ratio']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ddd38-4417-47ff-8913-23084fe59ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcfcda-4ba1-40d7-a221-bf3ea46215d8",
   "metadata": {},
   "source": [
    "## Task 1: Load the 'Experiments' sheet. How many rows and columns are there?\n",
    "\n",
    "First, we need to load the data from the `Experiments` sheet. We can then use the .shape attribute to get the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9997a-74ac-4836-92df-efe1c9f00412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the 'Experiments' sheet (CSV file)\n",
    "experiments_df = pd.read_excel('data.xlsx', sheet_name='Experiments')\n",
    "\n",
    "# Use the .shape attribute to get the number of rows and columns\n",
    "rows, columns = experiments_df.shape\n",
    "print(rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7646-a025-480c-8510-55f3b92ba016",
   "metadata": {},
   "source": [
    "## Task 2: Show the first 8 rows and the last 3 rows.\n",
    "\n",
    "We can use the `.head()` and `.tail()` methods to view specific numbers of rows from the beginning and end of the DataFrame, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd9b8c-7d72-4a71-9b3b-1d658d4f8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 8 rows\n",
    "first_8_rows = experiments_df.head(8)\n",
    "\n",
    "# Show the last 3 rows\n",
    "last_3_rows = experiments_df.tail(3)\n",
    "\n",
    "print(first_8_rows)\n",
    "print(last_3_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2358267-684e-4b43-9afa-abd92172d24d",
   "metadata": {},
   "source": [
    "## Task 3: What are the dtypes? Convert 'dose_mg' to float and 'date' to datetime.\n",
    "\n",
    "The `.dtypes` attribute shows the data type of each column. We'll use `pd.to_numeric()` to handle the `dose_mg` column, which may contain non-numeric values, and `pd.to_datetime()` for the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d87cc-8e53-4745-ac00-d19618bbb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check original dtypes\n",
    "print(experiments_df.dtypes)\n",
    "\n",
    "# Convert 'dose_mg' to float\n",
    "experiments_df['dose_mg'] = pd.to_numeric(experiments_df['dose_mg'])\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "experiments_df['date'] = pd.to_datetime(experiments_df['date'])\n",
    "\n",
    "# Check new dtypes\n",
    "print()\n",
    "print(experiments_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5da5a1-d6e3-4140-91b1-da249beb376f",
   "metadata": {},
   "source": [
    "## Task 4: How many missing values are there in each column? Fill missing 'response' with the group median per 'treatment'.\n",
    "\n",
    "We'll use `.isnull().sum()` to count missing values. For filling the missing 'response' values, we'll apply a common strategy of using the median of the corresponding treatment group, which is a more robust approach than using the overall median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad150b3-4d2f-4cfe-8c99-c01d70cb8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "missing_values_count = experiments_df.isnull().sum()\n",
    "\n",
    "# Fill missing 'response' values with the median of their 'treatment' group\n",
    "experiments_df['response'] = experiments_df['response'].fillna(\n",
    "    experiments_df.groupby('treatment')['response'].transform('median')\n",
    ")\n",
    "\n",
    "print(f\"Original missing values:\\n{missing_values_count}\\n\\nMissing values after filling:\\n{experiments_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c0660-23d0-42ab-8a53-9b897ccee913",
   "metadata": {},
   "source": [
    "## Task 5: Strip whitespace and lowercase the 'species' values. Count unique species.\n",
    "\n",
    "The `.str` accessor in Pandas allows us to apply string methods to an entire `Series`. We'll chain the `.str.strip()` and `.str.lower()` methods. Then, we can use `value_counts()` or `nunique()` to find the unique species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494c408-41aa-449f-af32-38ec69cc208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace and lowercase 'species' values\n",
    "experiments_df['species'] = experiments_df['species'].str.strip().str.lower()\n",
    "\n",
    "# Count the number of unique species\n",
    "unique_species_count = experiments_df['species'].nunique()\n",
    "\n",
    "# Show the counts for each unique species\n",
    "species_counts = experiments_df['species'].value_counts()\n",
    "\n",
    "print(f\"Number of unique species: {unique_species_count}\\n\\nCounts of each unique species:\\n{species_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599e16-e157-49cc-ace9-6d6af2895bd9",
   "metadata": {},
   "source": [
    "## Task 6: From 'Experiments', compute mean and std of 'response' by 'treatment' and 'dose_mg'.\n",
    "\n",
    "We'll use the powerful `.groupby()` method on two columns (`treatment` and `dose_mg`) and then use the `.agg()` method to apply both the mean and standard deviation functions to the `response` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913eddc-ff71-46a6-84c2-dff77bfbf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'treatment' and 'dose_mg', then aggregate 'response'\n",
    "agg_results = experiments_df.groupby(['treatment', 'dose_mg'])['response'].agg(['mean', 'std'])\n",
    "\n",
    "print(agg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea905f8-0aa5-4844-a8c7-e677fce1ab40",
   "metadata": {},
   "source": [
    "## Task 7: In 'Sensors_TimeSeries', resample each sensor to 30-minute means. Forward-fill gaps up to 2 periods.\n",
    "\n",
    "This task requires us to work with time-series data. We'll first load the `Sensors_TimeSeries` data and convert the `timestamp_utc` column to a proper datetime object. Then, we can set it as the index and use `.resample()` to group the data into 30-minute intervals and calculate the mean. Finally, we'll use `.ffill(limit=2)` to forward-fill any small gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a043c-ed4c-477e-a25f-41aabdf8cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'Sensors_TimeSeries' data\n",
    "sensors_df = pd.read_excel('data.xlsx', sheet_name='Sensors_TimeSeries')\n",
    "print(sensors_df.head())\n",
    "\n",
    "# Convert timestamp_utc to datetime\n",
    "sensors_df['timestamp_utc'] = pd.to_datetime(sensors_df['timestamp_utc'])\n",
    "\n",
    "# Set the timestamp as the index and resample to 30-minute means\n",
    "resampled_data = sensors_df.set_index('timestamp_utc').groupby('sensor_id').resample('30min').mean()\n",
    "\n",
    "# Forward-fill gaps up to 2 periods\n",
    "resampled_data = resampled_data.ffill(limit=2)\n",
    "\n",
    "print()\n",
    "print(resampled_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53662a17-a182-4a25-b5d9-7c32cc9bcdde",
   "metadata": {},
   "source": [
    "## Task 8: Merge 'Gene_Expression_Wide' with 'Sample_Metadata' on sample_id (columns starting with 'S'). Bring 'condition' and 'batch' alongside expression values.\n",
    "\n",
    "We need to merge the `Gene_Expression_Wide` and `Sample_Metadata` DataFrames on a common `sample_id` column. We'll use `pd.merge()` for this and specify the columns to merge on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a3e2a-7fd6-4486-8dc6-7d4d29ae196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary sheets\n",
    "gene_expression_df = pd.read_excel('data.xlsx', sheet_name='Gene_Expression_Wide')\n",
    "sample_metadata_df = pd.read_excel('data.xlsx', sheet_name='Sample_Metadata')\n",
    "\n",
    "# Merge the dataframes on the 'sample_id' columns\n",
    "# Note: In 'Gene_Expression_Wide', the sample IDs are columns, so we need to transpose it first\n",
    "# or melt it to a long format. A simpler approach is to merge the metadata into a temporary\n",
    "# transposed version and then join. A much cleaner way is to first melt the gene expression data.\n",
    "# For simplicity, we'll use a direct merge with a re-shaped dataframe.\n",
    "# Let's adjust the sample_metadata to have 'sample_id' as the index for easier joining.\n",
    "sample_metadata_df.set_index('sample_id', inplace=True)\n",
    "gene_expression_df.rename(columns={'gene': 'gene_id'}, inplace=True)\n",
    "gene_expression_df.set_index('gene_id', inplace=True)\n",
    "merged_df = gene_expression_df.transpose().merge(sample_metadata_df[['condition', 'batch']], left_index=True, right_index=True)\n",
    "\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82bb6d-9050-4107-9177-f3e1737cf223",
   "metadata": {},
   "source": [
    "## Task 9: From 'Assays_Long', pivot to wide with one row per (sample_id, day) and columns as assay_type with mean 'value'.\n",
    "\n",
    "The `Assays_Long` data is in a long format. We need to reshape it into a wide format where each `assay_type` is its own column. Since we need to calculate the mean value for cases with duplicate `(sample_id, day, assay_type)` combinations, `pd.pivot_table()` is the appropriate function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c7ded-844f-494f-95c4-8d2b93d199a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'Assays_Long' data\n",
    "assays_df = pd.read_excel('data.xlsx', sheet_name='Assays_Long')\n",
    "\n",
    "# Pivot the data to wide format, computing the mean value\n",
    "pivoted_df = assays_df.pivot_table(\n",
    "    index=['sample_id', 'day'],\n",
    "    columns='assay_type',\n",
    "    values='value',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(pivoted_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758c6ce-9ca6-42a1-bfc9-78014e9c92c4",
   "metadata": {},
   "source": [
    "## Task 10: Join 'Geo_Sites' to 'Experiments' using 'site_id'. Compute average response by region.\n",
    "\n",
    "We will merge the `Geo_Sites` DataFrame with the cleaned `Experiments` DataFrame on the `site_id` column. After the merge, we can use `groupby()` to calculate the average response for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac55e8-e4a9-4363-a0c8-d813200499fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'Geo_Sites' data\n",
    "geo_sites_df = pd.read_excel('data.xlsx', sheet_name='Geo_Sites')\n",
    "\n",
    "# Merge the two dataframes on 'site_id'\n",
    "experiments_with_geo = pd.merge(experiments_df, geo_sites_df, on='site_id', how='left')\n",
    "\n",
    "# Compute the average response per region\n",
    "average_response_by_region = experiments_with_geo.groupby('region')['response'].mean()\n",
    "\n",
    "print(average_response_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0277b-1f61-4466-a4fb-c9aa67133ac5",
   "metadata": {},
   "source": [
    "## Task 11: In 'Inventory' and 'Inventory_Updates', perform a left join on 'sku'. Compute the new stock = stock + delta (NaN -> 0).\n",
    "\n",
    "This task combines merging and handling missing values. We'll perform a left merge from `Inventory` to `Inventory_Updates`. The `delta` column in the merged DataFrame will have `NaN` values for items without updates. We will fill these `NaN`s with 0 before calculating the `new_stock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633eb61-98ab-4da9-a265-ad12bfbe412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary sheets\n",
    "inventory_df = pd.read_excel('data.xlsx', sheet_name='Inventory')\n",
    "updates_df = pd.read_excel('data.xlsx', sheet_name='Inventory_Updates')\n",
    "\n",
    "# Perform a left merge on 'sku'\n",
    "inventory_merged = pd.merge(inventory_df, updates_df, on='sku', how='left')\n",
    "\n",
    "# Fill NaN values in 'delta' with 0\n",
    "inventory_merged['delta'] = inventory_merged['delta'].fillna(0)\n",
    "\n",
    "# Compute the new stock\n",
    "inventory_merged['new_stock'] = inventory_merged['stock'] + inventory_merged['delta']\n",
    "\n",
    "print(inventory_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065a439-05a1-4acb-9e4d-ab43d3fd912d",
   "metadata": {},
   "source": [
    "## Task 12: Detect duplicated rows in 'Experiments' (full-row duplicates) and drop them keeping the first occurrence.\n",
    "\n",
    "Pandas' `drop_duplicates()` method is perfect for this. It can detect and remove rows that are identical across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2102c5-f7cb-4acb-b999-817553e834be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count original rows\n",
    "original_rows = len(experiments_df)\n",
    "\n",
    "# Drop duplicate rows, keeping the first occurrence\n",
    "df_no_dupes = experiments_df.drop_duplicates()\n",
    "\n",
    "# Count rows after dropping duplicates\n",
    "rows_after_dupes = len(df_no_dupes)\n",
    "\n",
    "print(f\"Original number of rows: {original_rows}\",\n",
    "      f\"Number of rows after dropping duplicates: {rows_after_dupes}\", \n",
    "f\"Dropped {original_rows - rows_after_dupes} duplicate rows.\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab32749-3b99-4134-84bb-4e7719a15266",
   "metadata": {},
   "source": [
    "## Task 13: Parse the 'notes' column in 'Experiments' and extract any temperature pattern like 'T=23.5C' into a new column 'temp_C'.\n",
    "\n",
    "We'll use the `.str.extract()` method with a regular expression to find and capture the temperature value. The regular expression `r'T=(\\d+\\.?\\d*)C'` is designed to match a temperature pattern, capturing the number part, and it also handles cases where there's no decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0d7a9-b50a-49dd-b6fd-e24171b9a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the temperature pattern into a new column\n",
    "experiments_df['temp_C'] = experiments_df['notes'].str.extract(r'T=(\\d+\\.?\\d*)C')\n",
    "\n",
    "# Convert the new column to a numeric type\n",
    "experiments_df['temp_C'] = pd.to_numeric(experiments_df['temp_C'])\n",
    "\n",
    "print(experiments_df[['notes', 'temp_C']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69a7f2-4d30-49ac-9741-e1cb3b706694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
